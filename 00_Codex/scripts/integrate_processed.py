#!/usr/bin/env python3
"""
Integrate processing results into Codex Database
"""

import sys
import re
from pathlib import Path
from typing import List

# Setup paths
script_dir = Path(__file__).parent
project_root = script_dir.parent
sys.path.insert(0, str(script_dir))

from database import CodexDatabase

def extract_tags_from_summary(content: str) -> List[str]:
    """Extract tags from Key Takeaways or detailed analysis"""
    tags = set()
    
    # Try to find specific mentioned entities or keywords in Key Takeaways
    match = re.search(r'## Key Takeaways(.*?)(##|$)', content, re.DOTALL)
    if match:
        takeaways = match.group(1)
        # Extract capitalized words that might be entities (simple heuristic)
        # This is basic; for better results, we would use an LLM or NLP lib
        # But here we want to be fast and local.
        
        # Strategy: Look for specific business/tech keywords
        keywords = ["AI", "IA", "Startup", "Investment", "Capital", "Defense", "Biotech", 
                    "Infrastructure", "SaaS", "Growth", "Venture", "Market", "Tech", "Brasil", "Brazil", "USA"]
        
        for kw in keywords:
            if re.search(r'\b' + re.escape(kw) + r'\b', takeaways, re.IGNORECASE):
                tags.add(kw.lower())
                
    return list(tags)

def main():
    db = CodexDatabase()
    processed_dir = project_root / "eximia_data" / "02_PROCESSED"
    
    print(f"üîç Scanning {processed_dir}...")
    
    if not processed_dir.exists():
        print("‚ùå Processed directory not found")
        return

    processed_files = list(processed_dir.glob("*_processed*.md"))
    print(f"üìÑ Found {len(processed_files)} processed files")
    
    for p_file in processed_files:
        print(f"\n‚öôÔ∏è  Processing: {p_file.name}")
        
        # Get original filename matching
        # Remove known suffixes
        stem = p_file.stem
        for suffix in ["_processed_sonnet", "_processed_ollama", "_processed"]:
            if stem.endswith(suffix):
                stem = stem[:-len(suffix)]
                break
        
        original_name = stem + ".md"
        
        # Find in DB by title or file path pattern
        # Since we don't have the exact original path handy without searching, let's search DB
        # Searching by file_path ending with original_name
        
        conn = db._get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM contents WHERE file_path LIKE ?", (f"%{original_name}",))
        row = cursor.fetchone()
        
        if not row:
            print(f"   ‚ö†Ô∏è  Original content not found in DB for: {original_name}")
            # Try searching by title derived from filename?
            # stem: andreessen_horowitz... 
            # This relies on the file path being exact.
            continue
            
        content_id = row['id']
        print(f"   ‚úÖ Linked to ID: {content_id}")
        
        # 1. Register processing history
        # Check if already registered
        cursor.execute(
            "SELECT id FROM processing_history WHERE content_id = ? AND output_path = ?", 
            (content_id, str(p_file))
        )
        if cursor.fetchone():
            print("   ‚ÑπÔ∏è  Already registered in history")
        else:
            db.add_processing(
                content_id=content_id,
                agent="process_with_ollama",
                action="summary_lx",
                output_path=str(p_file),
                status="success",
                notes="Generated by qwen2.5:14b"
            )
            print("   üíæ Added to processing history")
            
        # 2. Update Categorization (Tags)
        # Read processed content to extract potential tags
        try:
            p_content = p_file.read_text(encoding='utf-8')
            new_tags = extract_tags_from_summary(p_content)
            
            if new_tags:
                print(f"   üè∑Ô∏è  Extracted tags: {new_tags}")
                # Insert tags
                for tag in new_tags:
                    cursor.execute("""
                        INSERT OR IGNORE INTO content_tags (content_id, tag_name)
                        VALUES (?, ?)
                    """, (content_id, tag))
                conn.commit()
                print("   ‚úÖ Tags updated")
            else:
                print("   ‚ö†Ô∏è  No tags extracted")
                
        except Exception as e:
            print(f"   ‚ùå Error updating tags: {e}")
            
        conn.close()

if __name__ == "__main__":
    main()
