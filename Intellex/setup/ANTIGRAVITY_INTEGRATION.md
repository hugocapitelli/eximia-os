# Como Antigravity Chama o LLM Local

## ğŸ¯ Fluxo Completo

```
VocÃª (no chat): "Processe o livro Naval Ravikant com LLM local"
    â†“
Antigravity: Entende o pedido
    â†“
Antigravity: run_command â†’ python local_llm_processor.py
    â†“
Script Python: Chama Ollama (qwen2.5:32b)
    â†“
Ollama: Gera resposta (5-15min)
    â†“
Script: Salva em arquivo .md
    â†“
Antigravity: LÃª resultado e mostra para vocÃª
```

---

## ğŸ“ Exemplo de Uso via Antigravity

### VocÃª diz no chat:

```
@antigravity Processe o livro Naval Ravikant usando LLM local (Qwen2.5 32B) com o prompt_v3_optimized
```

### Antigravity executa:

```python
# Antigravity internamente farÃ¡ algo como:

# 1. Salvar prompt em arquivo temporÃ¡rio
with open("temp_prompt.txt", "w") as f:
    f.write(prompt_v3_optimized_content)

# 2. Executar script via run_command
result = run_command(
    "python c:/Users/hugoc/OneDrive/.../Intellex/scripts/local_llm_processor.py "
    "--prompt temp_prompt.txt "
    "--output naval_synthesis_local.md "
    "--model qwen2.5:32b "
    "--temperature 1.0 "
    "--max-tokens 8192"
)

# 3. Aguardar conclusÃ£o (5-15min)

# 4. Ler resultado
with open("naval_synthesis_local.md") as f:
    synthesis = f.read()

# 5. Mostrar para vocÃª
print(f"âœ… Processamento completo! {len(synthesis.split())} palavras geradas")
```

---

## ğŸš€ Comandos que VocÃª Pode Usar

### Comando 1: Processar livro completo

```
Processe o livro "O Almanaque de Naval Ravikant" usando:
- LLM: Qwen2.5 32B local
- Prompt: Intellex/prompts/prompt_v3_optimized.md
- Output: Intellex/outputs/naval_ravikant_lx_local/deep_synthesis.md
```

### Comando 2: Multi-pass local

```
Execute multi-pass do Naval Ravikant:
1. Liste frameworks (Gemini)
2. Expanda cada framework (Qwen2.5 32B local)
3. Compile synthesis (Gemini)
```

### Comando 3: Hybrid approach

```
Process Naval usando hybrid:
- Estrutura: Gemini
- Frameworks expansion: Qwen2.5 72B local
- Final compilation: Gemini
```

---

## ğŸ”§ Uso Manual (se preferir rodar direto)

### Passo 1: Preparar prompt

```powershell
# Copiar prompt otimizado
cp Intellex/prompts/prompt_v3_optimized.md temp_prompt.txt
```

### Passo 2: Executar script

```powershell
cd "c:\Users\hugoc\OneDrive\Ãrea de Trabalho\exÃ­mIA Ventures\eximIA.OS"

python Intellex/scripts/local_llm_processor.py `
  --prompt temp_prompt.txt `
  --output Intellex/outputs/naval_local/synthesis.md `
  --model qwen2.5:32b `
  --temperature 1.0 `
  --max-tokens 8192
```

### Passo 3: Aguardar (5-15 min)

VocÃª verÃ¡ progresso:
```
ğŸš€ Iniciando processamento com qwen2.5:32b...
âœ… Prompt carregado: 15234 caracteres
âš™ï¸ ConfiguraÃ§Ãµes:
   - Temperature: 1.0
   - Max tokens: 8192
ğŸ”„ Gerando resposta...
   ğŸ“Š Progresso: 100 tokens, ~75 palavras
   ğŸ“Š Progresso: 200 tokens, ~155 palavras
   ...
   ğŸ“Š Progresso: 3500 tokens, ~2800 palavras
âœ… GeraÃ§Ã£o completa!
   ğŸ“ Palavras: 2847
   ğŸ“ Tokens: 3542
ğŸ’¾ Output salvo em: synthesis.md
```

### Passo 4: Verificar resultado

```powershell
(Get-Content Intellex/outputs/naval_local/synthesis.md | Measure-Object -Word).Words
```

**Expectativa com Qwen2.5 32B:** 2.800-4.000 palavras (110-150% do Claude)

---

## ğŸ“Š ComparaÃ§Ã£o de Abordagens

| Abordagem | Qualidade | Tempo | AutomaÃ§Ã£o | Custo |
|-----------|-----------|-------|-----------|-------|
| **Gemini manual** | 60% | 5min | âŒ | GrÃ¡tis |
| **Gemini optimized** | 70% | 10min | âŒ | GrÃ¡tis |
| **Qwen 32B local** | 100%+ | 10-15min | âœ… via Antigravity | GrÃ¡tis (apÃ³s setup) |
| **Qwen 72B local** | 120%+ | 20-30min | âœ… via Antigravity | GrÃ¡tis (GPU 48GB+) |
| **Claude API** | 100% | 5min | âœ… | $$ caro |

---

## ğŸ¯ Workflow Recomendado

### Para livros crÃ­ticos (Naval, Principles, etc):

```
1. Antigravity: "Liste frameworks do Naval usando Gemini"
   â†’ RÃ¡pido, Gemini Ã© bom nisso

2. Antigravity: "Expanda cada framework usando Qwen 32B"
   â†’ 10 calls Ã— Qwen = MÃ¡xima densidade

3. Antigravity: "Compile synthesis final com Gemini"
   â†’ Gemini junta tudo

RESULTADO: 4.000-5.000 palavras, 120-150% Claude
```

### Para livros exploratÃ³rios:

```
Antigravity: "Processe rÃ¡pido com Qwen 14B"
â†’ 1 call, ~10min, 85-90% Claude
```

---

## ğŸ”„ IntegraÃ§Ã£o com Eximia Runtime (Futuro)

Eventualmente vocÃª poderÃ¡ fazer:

```bash
eximia run intellex \
  --module book_processor \
  --llm qwen2.5:32b \
  --input naval.pdf \
  --output naval_lx/
```

Mas por enquanto, Antigravity executa via `local_llm_processor.py`.

---

## ğŸ’¡ Dicas de Performance

### Para mÃ¡xima qualidade:
```
--model qwen2.5:72b --temperature 1.2
```

### Para balanceado:
```
--model qwen2.5:32b --temperature 1.0
```

### Para rÃ¡pido:
```
--model qwen2.5:14b --temperature 0.8
```

---

## ğŸ› Troubleshooting

### Erro: "Connection refused"
**Causa:** Servidor Ollama nÃ£o estÃ¡ rodando  
**SoluÃ§Ã£o:**
```powershell
ollama serve
```

### Erro: "Model not found"
**Causa:** Modelo nÃ£o foi baixado  
**SoluÃ§Ã£o:**
```powershell
ollama pull qwen2.5:32b
```

### Erro: "Out of memory"
**Causa:** Modelo muito grande para GPU  
**SoluÃ§Ã£o:** Use modelo menor
```powershell
ollama pull qwen2.5:14b
# E no comando:
--model qwen2.5:14b
```

---

## ğŸ‰ PrÃ³ximo Passo

ApÃ³s setup completo:

1. âœ… Ollama instalado (SETUP_LOCAL_LLM.md)
2. âœ… Script Python criado (local_llm_processor.py)
3. âœ… Entendeu como Antigravity chama (este arquivo)

**TESTE AGORA:**

```
@antigravity Execute este comando:

python "c:\Users\hugoc\OneDrive\Ãrea de Trabalho\exÃ­mIA Ventures\eximIA.OS\Intellex\scripts\local_llm_processor.py" --prompt "c:\Users\hugoc\OneDrive\Ãrea de Trabalho\exÃ­mIA Ventures\eximIA.OS\Intellex\prompts\prompt_v3_optimized.md" --output "c:\Users\hugoc\OneDrive\Ãrea de Trabalho\exÃ­mIA Ventures\eximIA.OS\Intellex\outputs\naval_local_test\synthesis.md" --model qwen2.5:32b
```

Antigravity executarÃ¡ e vocÃª verÃ¡ o Qwen processando em tempo real!
