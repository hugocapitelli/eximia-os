# Setup Local LLM (Qwen2.5) para Intellex

## üéØ Objetivo

Instalar Qwen2.5 localmente e permitir que Antigravity chame automaticamente para processar livros.

---

## Passo 1: Instalar Ollama

### Windows (Recomendado)

1. **Download Ollama:**
   - V√° em: https://ollama.ai/download/windows
   - Baixe e instale `OllamaSetup.exe`

2. **Verificar instala√ß√£o:**
```powershell
ollama --version
```

Deve mostrar: `ollama version X.X.X`

---

## Passo 2: Escolher e Baixar Modelo

### Op√ß√µes de Modelos (escolha baseado no seu hardware)

| Modelo | VRAM Necess√°ria | RAM | Qualidade | Velocidade |
|--------|-----------------|-----|-----------|------------|
| **qwen2.5:72b** | 48GB+ GPU | 96GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Melhor | üê¢ Lento |
| **qwen2.5:32b** | 24GB GPU | 48GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚ö° M√©dio |
| **qwen2.5:14b** | 12GB GPU | 24GB+ | ‚≠ê‚≠ê‚≠ê Muito bom | ‚ö°‚ö° R√°pido |
| **qwen2.5:7b** | 8GB GPU | 16GB+ | ‚≠ê‚≠ê Bom | ‚ö°‚ö°‚ö° Muito r√°pido |

**Recomenda√ß√£o:** Se tem GPU boa, use **32b ou 72b**. Se n√£o, use **14b**.

### Download do Modelo

```powershell
# Escolha UM destes comandos baseado no seu hardware:

# Para GPU 48GB+ (melhor qualidade)
ollama pull qwen2.5:72b

# Para GPU 24GB (balanceado) - RECOMENDADO
ollama pull qwen2.5:32b

# Para GPU 12GB (r√°pido)
ollama pull qwen2.5:14b

# Para CPU ou GPU 8GB (mais leve)
ollama pull qwen2.5:7b
```

**Tempo de download:** 10-60 minutos dependendo do modelo

---

## Passo 3: Testar o Modelo

```powershell
# Iniciar servidor Ollama (roda em background)
ollama serve

# Em outro terminal, testar:
ollama run qwen2.5:32b "Ol√°, voc√™ funciona?"
```

Se responder algo coerente, est√° funcionando! ‚úÖ

---

## ‚ö†Ô∏è VRAM vs RAM (O Que Acontece se Faltar GPU?)

**D√∫vida Comum:** *"Tenho GPU de 8GB, posso rodar o de 32GB?"*

**Resposta:** **SIM, mas com penalidade de velocidade.**

Ollama usa uma tecnologia chamada **Offloading Inteligente**:
1. Ele enche a **VRAM** (Mem√≥ria da Placa de V√≠deo) at√© o limite.
2. O que sobrar, ele joga para a **RAM do Sistema** (CPU).
3. O processamento acontece dividido entre GPU e CPU.

### Impacto na Performance

| Cen√°rio | Onde roda o modelo | Velocidade Estimada | Usabilidade |
|---------|--------------------|---------------------|-------------|
| **100% na GPU** | Cabe tudo na VRAM | **30-50 tokens/s** | ‚ö° Instant√¢neo |
| **50% GPU / 50% CPU** | Metade na VRAM, metade RAM | **3-6 tokens/s** | üê¢ Lento, mas us√°vel |
| **100% na CPU** | Nada na VRAM | **1-2 tokens/s** | üêå Muito lento |

**Exemplo Pr√°tico (Qwen 32B - aprox 20GB):**
- **RTX 3090 (24GB):** Cabe 100% ‚úÖ (Super r√°pido)
- **RTX 3060 (12GB):** ~60% na GPU, 40% na RAM ‚ö†Ô∏è (Funciona, mas velocidade cai 80%)

### Solu√ß√£o: Quantiza√ß√£o (Diminuir o modelo)
Se o modelo for pesado, use uma vers√£o "comprimida" (quantizada). A perda de intelig√™ncia √© m√≠nima, mas o ganho de efici√™ncia √© enorme.

```powershell
# Qwen 32B Original (FP16) = ~60GB (Imposs√≠vel para maioria)
# Qwen 32B Padr√£o do Ollama (Q4_0) = ~19GB (Cabe em GPUs top)
# Qwen 32B Comprimido (Q2_K) = ~12GB (Cabe em GPUs m√©dias)
```

Para baixar vers√µes menores, procure pelas tags no site do Ollama (ex: `qwen2.5:32b-q2_k`).

---

## Passo 4: Integra√ß√£o com Intellex

### 4.1 Instalar depend√™ncias Python

```powershell
pip install ollama requests
```

### 4.2 Verificar que Antigravity pode executar Python

O script que vou criar usa Python. Antigravity executar√° via `run_command`.

---

## Passo 5: Configura√ß√£o Completa

Ap√≥s seguir os passos acima, voc√™ ter√°:

‚úÖ Ollama instalado e rodando
‚úÖ Qwen2.5 baixado (tamanho escolhido)
‚úÖ Servidor rodando em `http://localhost:11434`
‚úÖ Pronto para Antigravity chamar via script Python

---

## üìä Expectativas de Performance

### Qwen2.5 72B
- **Qualidade:** Pode SUPERAR Claude em densidade
- **Velocidade:** ~10-20 tokens/s (com GPU 48GB)
- **Uso:** Livros cr√≠ticos, synthesis m√°xima

### Qwen2.5 32B
- **Qualidade:** Equivalente a Claude
- **Velocidade:** ~20-40 tokens/s (com GPU 24GB)
- **Uso:** Balanceado, recomendado

### Qwen2.5 14B
- **Qualidade:** 85-90% do Claude
- **Velocidade:** ~40-80 tokens/s (com GPU 12GB)
- **Uso:** Processamento r√°pido

---

## üîß Troubleshooting

### Erro: "ollama: command not found"
**Solu√ß√£o:** Reinicie o terminal ap√≥s instala√ß√£o ou adicione ao PATH:
```powershell
$env:Path += ";C:\Users\$env:USERNAME\AppData\Local\Programs\Ollama"
```

### Erro: "Out of memory"
**Solu√ß√£o:** Modelo muito grande para seu hardware. Baixe modelo menor:
```powershell
ollama pull qwen2.5:14b
```

### Erro: "Connection refused"
**Solu√ß√£o:** Servidor Ollama n√£o est√° rodando. Execute:
```powershell
ollama serve
```

---

## üöÄ Pr√≥ximos Passos

Ap√≥s completar este setup:
1. ‚úÖ Ollama instalado
2. ‚úÖ Modelo baixado
3. ‚úÖ Servidor rodando

**V√° para:** `local_llm_processor.py` (pr√≥ximo arquivo)

L√° est√° o script que Antigravity executar√° para chamar o Qwen2.5 automaticamente.

---

## üìù Configura√ß√µes Avan√ßadas (Opcional)

### Aumentar contexto m√°ximo
```powershell
# Editar Modelfile para aumentar context window
ollama show qwen2.5:32b --modelfile > Modelfile
# Editar e adicionar: parameter num_ctx 16384
ollama create qwen2.5-extended -f Modelfile
```

### Quantiza√ß√£o customizada (economizar VRAM)
```powershell
# Q4 (mais r√°pido, menos precis√£o)
ollama pull qwen2.5:32b-q4_K_M
```

---

## ‚ö° Quick Start

**TL;DR - Cole estes comandos se tiver GPU 24GB+:**

```powershell
# 1. Download instalador Ollama de ollama.ai
# 2. Instalar
# 3. Rodar isto:

ollama serve
# (novo terminal)
ollama pull qwen2.5:32b
ollama run qwen2.5:32b "Teste"

# Se funciona, v√° para local_llm_processor.py

---

## üéØ Recomenda√ß√£o para SEU Hardware (16GB VRAM + 32GB RAM)

Com essa configura√ß√£o, voc√™ est√° no "Sweet Spot" para rodar modelos de alta intelig√™ncia com velocidade aceit√°vel.

### A Melhor Escolha: Qwen 2.5 32B (Quantiza√ß√£o q4_K_M)
Este √© o modelo que compete com o Claude Sonnet.

- **Tamanho do Modelo:** ~20GB.
- **Como vai rodar:**
  - **16GB** v√£o para sua GPU (VRAM) ‚Üí Processamento R√°pido.
  - **~4GB** v√£o para sua RAM ‚Üí Processamento mais lento.
- **Resultado:** Voc√™ ter√° a intelig√™ncia m√°xima do 32B, rodando a uma velocidade mista (~8-12 tokens/segundo). √â **perfeito** para processamento de livros onde qualidade importa mais que lat√™ncia instant√¢nea.

**Comando para baixar:**
```powershell
ollama run qwen2.5:32b
```
*(O Ollama baixa automaticamente a vers√£o q4_0 ou q4_K_M que √© ideal para voc√™)*

### Alternativa Ultra-R√°pida: Qwen 2.5 14B
Se voc√™ quiser velocidade extrema (ex: para chat em tempo real), use o 14B. Ele caber√° 100% na sua VRAM.

**Comando:**
```powershell
ollama run qwen2.5:14b
```

```
